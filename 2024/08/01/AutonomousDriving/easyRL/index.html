<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>强化学习 | 11的Blog</title><meta name="keywords" content="自动驾驶,强化学习,easyRL蘑菇书"><meta name="author" content="11andyy"><meta name="copyright" content="11andyy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="强化学习"><meta name="application-name" content="强化学习"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="强化学习"><meta property="og:url" content="https://11andyy.github.io/2024/08/01/AutonomousDriving/easyRL/index.html"><meta property="og:site_name" content="11的Blog"><meta property="og:description" content="强化学习基础到目前为止：  学习，需要大量的专家演示 使用辅助短期损失函数 模仿学习：动作的每帧损失 直接感知：可供性指标的每帧损失  现在： ​    基于我们真正关心的损失模型  最小化到达目标的时间 最小化碰撞次数 最小化风险  学习的类型：  监督学习 数据集：&amp;#123;(xi,yi)&amp;amp;#"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/d7b8a1da4176723d2a85993fae53b10a37d7ea4f.png"><meta property="article:author" content="11andyy"><meta property="article:tag" content="11的Blog"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/d7b8a1da4176723d2a85993fae53b10a37d7ea4f.png"><meta name="description" content="强化学习基础到目前为止：  学习，需要大量的专家演示 使用辅助短期损失函数 模仿学习：动作的每帧损失 直接感知：可供性指标的每帧损失  现在： ​    基于我们真正关心的损失模型  最小化到达目标的时间 最小化碰撞次数 最小化风险  学习的类型：  监督学习 数据集：&amp;#123;(xi,yi)&amp;amp;#"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="https://11andyy.github.io/2024/08/01/AutonomousDriving/easyRL/"><link rel="preconnect" href="//cdn.cbd.int"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: {"enable":true,"title":"与数百名博主无限进步","addFriendPlaceholder":"昵称（请勿包含博客等字样）：\n网站地址（要求博客地址，请勿提交个人主页）：\n头像图片url（请提供尽可能清晰的图片，我会上传到我自己的图床）：\n描述：\n站点截图（可选）：\n"},
  peoplecanvas: undefined,
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"tianli","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"不要走！再看看嘛！","backTitle":"欢迎肥来！"},
  LA51: undefined,
  greetingBox: {"enable":true,"default":"晚上好👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋, 小猪","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/',
  preloader: {"source":3},
  friends_vue_info: {"apiurl":null},
  navMusic: true,
  mainTone: undefined,
  authorStatus: {"skills":["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: 11andyy","link":"链接: ","source":"来源: 11的Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: {"enable":true,"delay":100,"shiftDelay":200},
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: '11的Blog',
  title: '强化学习',
  postAI: '',
  pageFillDescription: '强化学习基础, 强化学习概述, 强化学习与监督学习, 强化学习的应用, 序列决策, 智能体与环境, 奖励, 序列决策, 动作空间, 强化学习智能体的组成成分和类型, 策略, 价值函数, 模型, 强化学习智能体的类型, 基于价值的智能体与基于策略的智能体, 有模型强化学习智能体与免模型强化学习智能体, 学习和规划, 探索和利用, 强化学习实验平台, Gym, MountainCar-v0, 关键词, 马尔可夫决策过程, 马尔可夫过程（MP）, 马尔可夫性质, 马尔可夫链, 马尔可夫奖励过程（MRP）, 回报与价值函数, 贝尔曼方程, 全期望公式, 贝尔曼方程的推导, 马尔可夫决策过程强化学习基础到目前为止学习需要大量的专家演示使用辅助短期损失函数模仿学习动作的每帧损失直接感知可供性指标的每帧损失现在基于我们真正关心的损失模型最小化到达目标的时间最小化碰撞次数最小化风险学习的类型监督学习数据集数据标签目标学习映射示例分类回归模仿学习可供性学习无监督学习数据集数据目标发现数据的底层结构示例聚类降维特征学习强化学习代理与提供数字奖励信号的环境交互目标学习如何采取行动以最大化奖励示例学习操纵或控制任务所有交互强化学习概述强化学习讨论的问题是智能体怎么在复杂的不确定的环境中最大化它能获得的奖励强化学习由两部分组成智能体和环境在强化学习过程中智能体和环境一直交互智能体在环境中获取某个状态后利用该状态输出一个动作这个动作也称为决策然后该动作在环境中被执行环境根据智能体采取的动作输出下一个状态和当前动作带来的奖励智能体的目的就是尽可能多地从环境中获取奖励在时间内观察环境状态在时间结束对环境的操作环境将奖励和新状态返回给目标选择动作以最大化奖励行动可能会产生长期影响回报可能是延迟的而不是及时的牺牲及时回报获得长期回报可能更好强化学习与监督学习以图片分类为例监督学习假设我们有大量被标注的数据这些图片独立同分布之间没有关联关系训练一个分类器在训练过程中需要正确的的标签传给圣经网络当做出错误预测时会告诉网络预测错误根据类似错误写出一个损失函数通过反向传播来训练神经网络在监督学习中有两个阶段输入的数据之间是没有关联的需要告诉学习器正确的标签是什么在强化学习中监督学习的两个假设都不成立以雅达利游戏为例打砖块智能体游戏过程中得到的观测不是独立同分布的上一帧和下一帧有非常强的连续性得到的数据是相关的时间序列数据另外我们没有得到立刻反馈游戏并没有告诉我们哪个动作是正确的因此强化学习的困难之处便是智能体不能得到及时的反馈但是我们依然希望智能体能在这个环境中学习因此强化学习的训练数据是一个玩游戏的过程从第一步开始采取一个动作第二步又采取一个动作得到的是一个玩游戏的序列假设当前是第三步我们把当前的序列放进网络希望网络可以得到一个输出动作但是我们没有标签说明输出的动作是正确错误必须等游戏结束才会知道这就面临着延迟奖励问题强化学习和监督学习的区别强化学习输入的样本是序列数据而不像监督学习中样本是独立的学习器并没有告诉每一步正确的动作是什么学习器需要自己去发现那些动作可以带来更多的奖励只有通过不断地尝试才能找到最有利的动作智能体获得自己能力的过程其实是不断试错探索的过程探索和利用是强化学习中的核心问题探索是尝试一些新的动作这些新的动作可能使我们得到更多地奖励也可能一无所有利用是采取已知的可以获得最多奖励的动作重复执行这个动作因此我们需要在探索和利用之间权衡在强化学习中没有非常强的监督者只有奖励信号并且奖励信号是延迟的即环境在很久以后才会告诉我们奖励是不是有效的强化学习的特点强化学习会试错探索通过探索环境来获取对环境的理解强化学习智能体会从环境里面获得延迟的奖励在强化学习的训练过程中时间非常重要因为我们获得的是有时间关联的数据而不是独立同分布的数据智能体的动作会影响它随后的到数据在强化学习中一个很重要的问题是如何让智能体的动作稳定提升强化学习的应用走路的智能体机械臂抓取魔方自动驾驶序列决策智能体与环境强化学习研究的问题实际上就是智能体和环境交互的问题智能体把动作输出到环境环境取得这个动作后进入下一步并把下一步的观测结果返回给智能体这样的交互会产生大量的观测智能体的目的就是从这些观测中学到最大化奖励的策略奖励奖励是由环境给的一种标量的反馈信号这种信号显示智能体在某一步骤采取某个策略的表现如何在不同的环境中奖励的标准也是不同的评判依据不一样序列决策在一个强化学习环境中智能体的目的就是选取一系列的动作来最大化奖励所以我们选取的动作必须有长期的影响但是在这个过程中智能体的奖励实际上是被延迟的在强化学习中一个重要的课题就是近期奖励和远期奖励的权衡研究如何让智能体获得更多的远期奖励在与环境交互的过程中智能体会获得很多观测针对每一个观测智能体会采取一个动作也会得到一个奖励所以历史是观测动作奖励的序列智能体在采取当前动作的时候会依赖与它之前得到的历史所以我们可以把整个游戏状态看成关于这个历史的函数状态和观测有什么关系状态是对世界的完整描述不会隐藏世界信息观测是对状态的部分描述可能会遗漏一些信息在深度强化学习中我们几乎总用实值的向量矩阵或更高阶的张量来表示状态和观测例如我们可以用像素值矩阵来表示一个视觉的观测用机器人关节角度和速度来表示一个机器人关节的状态环境有自己的函数来更新状态智能体自己的函数来更新状态重要当智能体的状态与环境状态等价时即当智能体能够观察到环境所有状态时我们称这个环境是完全可观测的在这种情况下强化学习通常被建模成一个马尔可夫决策过程问题在马尔可夫决策过程中上述公式表述为时刻智能体的所有观测时刻环境所有状态时刻智能体更新得到的所有状态但是有一种情况智能体得到的观测并不能包含环境运作的所有状态因为在强化学习的设定里面环境状态才会包含所有状态当智能体只能看到部分观测时就称这个环境是部分可观测的强化学习通常被建模为一个部分可观测的马尔可夫决策过程是的泛化部分可观测的马尔可夫决策过程依然具有马尔可夫的性质但是假设智能体无法感知环境的状态只能知道部分观测值比如在自动驾驶任务中智能体只能感知传感器采集的有限环境信息部分可观测马尔可夫决策过程可以用一个七元组表示状态空间为隐变量动作空间状态转移概率奖励函数观测概率观测空间折扣系数动作空间不同环境允许不同种类的动作在给定的环境中有效动作的集合经常被称为动作空间像围棋这种离散的动作空间中智能体的动作数量是有限的但是在连续的动作空间中动作是实值的向量例如走迷宫机器人如果只有往东往西往南往北这四种移动方式则动作空间为离散动作空间如果机器人可以任意角度移动则动作空间为连续动作空间强化学习智能体的组成成分和类型策略智能体会用策略选取下一步的动作价值函数用价值函数对当前状态进行评估价值函数用于评估智能体进入某个状态后可以对后面的奖励带来多大的影响价值函数越大说明智能体进入这个状态越有利模型模型表示智能体对环境状态进行理解决定了环境中世界的运行方式策略我的理解就是当观察到状态后在该状态应该采取什么样的动作的函数也就是说策略是一个从到的函数是采取的动作这个动作属于动作空间只要有了这个函数输入一个状态就会采取随机抽样来选取下一个动作这个函数实际上是一个概率密度函数表示在状态下采取动作的可能性策略是智能体的动作模型决定了智能体的动作它其实是一个函数用于把输入的状态变为动作策略分为两种随机性策略和决定性策略随机性策略就是函数即输入一个状态输出一个概率这个概率是智能体所有动作的概率然后对这个概率分布进行采样可得到智能体将要采取的动作比如概率往左概率往右通过采样就可以得到智能体将要采取的策略确定性策略就是智能体直接采取最有可能的动作即策略仅依赖当前的状态而不是整个历史但是当前的状态可能包括过去的观察结果如何学习策略模仿学习从专家演示中学习策略提供专家演示监督学习问题强化学习通过反复试验学习策略没有专家演示代理自己发现那些动作可以最大化预期未来奖励代理与环境交互并获得奖励代理发现更好的动作并改进策略从雅达利游戏来看策略函数的输入就是游戏的一帧他的输出能够决定智能体向左或向右移动通常情况下智能体使用随机性策略在学习中引入一定的随机性可以更好地探索环境随机性策略的动作具有多样性这一点在智能体博弈时非常最重要如果采用确定性策略很容易被博弈的对手猜到价值函数贝尔曼最优性取决于状态和学习取决于状态动作这部分的公式不是很理解价值函数是对未来奖励的预测用来评估状态的好坏价值函数里有一个折扣因子我们希望可以在尽可能短的时间里得到尽可能多的奖励比如现在我们有两个选择天后给我们块和现在立刻给块必然选择后者因为有利息因此我们可以把折扣因子放在价值函数中状态价值函数定义为对于所有的期望的下标是函数期望的值可以反应我们在状态下使用策略时到底可以获得多少奖励关于折扣因子当折扣因子小于时即时的奖励权重要大于长远的奖励还有一种定义的价值函数函数函数包括两个变量状态和动作定义为所以未来可以获得奖励的期望取决于当前状态和当前的动作函数是强化学习中要学习的一个函数因为当我们得到函数后进入某个状态要采取的最优动作可以通过函数得到模型模型决定了下一步的状态下一步的状态取决于当前的状态以及当前采取的动作它由状态转移概率和奖励函数两个部分组成状态转移概率为状态转移概率表示从状态采取动作转移到状态的概率奖励函数是指我们在当前状态采取了某个动作得以得到多大的奖励奖励函数当我们友链策略价值函数模型三个组成成分后就形成了一个马尔可夫决策过程这个过程可视化了状态之间的转移以及采取的动作例子走迷宫要求智能体从起点开始然后到终点的位置每走一步就会得到的奖励我们可以采取的动作是往上往下往左往右的动作我们用现在智能体所在的位置来描述当前的状态我们可以用不同的强化学习方法来解决这个环境如果我们采取基于策略的强化学习方法当学习好了这个环境后在每一个状态我们都会得到一个最佳的动作比如在第一个格子最佳动作是往右走通过最佳的策略我们可以最快地到达终点如果换成基于价值的强化学习方法利用价值函数作为导向我们就会的得到另外一种表征在每个状态返回一个价值比如我们在起点的价值是因为没走一步会最快步到达终点当我们快接近终点时这个数字变得越来越大在拐角时比如现在是智能体会看上下两个格子上面的价值变大了为下面的价值是因此它会选取往上走的策略通过学习价值的不同我们可以抽象出最佳的策略强化学习智能体的类型基于价值的智能体与基于策略的智能体基于价值的智能体显示地学习价值函数隐式地学习策略策略是从价值函数中推算出来的基于策略的智能体直接学习策略我们给它一个状态他会输出对应动作的概率基于策略的智能体并没有学习价值函数演员评论员智能体把两者结合起来把策略和价值函数都学习了通过两者交互得到最佳的动作基于策略和基于价值的强化学习方法有什么区别对于一个状态转移概率已知的马尔可夫决策过程我们可以使用动态规划算法来求解决策方式是只能个体在给定状态下从动作集合中选择一个动作的依据它是静态的不随状态变化而变化在基于策略的强化学习方式中智能体会制定一套动作策略并根据这个策略进行操作强化学习算法直接对这个策略进行优化使得定制的测录能够获得最大奖励在基于价值的强化学习方式中智能体不需要制定显示的策略而是维护一个价值表格或价值函数并通过这个价值表格或价值函数来选取价值最大的动作基于价值迭代的方法只能应用在不连续的离散的环境中比如围棋等基于策略迭代的方法可以选择连续的动作比如动作集合规模庞大动作连续的场景机器人控制领域基于价值的强化学习算法由学习等基于策略的强化学习方法有策略梯度算法此外演员评论员算法同时使用策略和价值评估做出决策其中智能体根据策略做出决策价值函数会对做出的动作给出价值这样可以在原有的策略梯度算法基础上加速学习过程取得更好的效果有模型强化学习智能体与免模型强化学习智能体有模型强化学习通过学习状态的转移才采取动作免模型强化学习智能体没有直接估计状态的转移也没有得到环境的具体转移变量它通过学习价值函数和策略函数进行决策免模型强化学习智能体的模型里面没有环境转移的模型我们可以用马尔可夫决策过程来定义强化学习任务并将其表示为四元组即状态集合动作集合状态转移函数和奖励函数如果这个四元组中的元素均已知且状态集合和动作集合在有限步数内是有限集则智能体可以根据真实环境进行建模构建一个虚拟的世界来模拟真实环境中的状态和交互反应具体来说当智能体知道状态转移函数和奖励函数后它就能知道在某一状态下执行某一动作后能带来的奖励和下一状态直接在虚拟世界学习规划策略然而在实际应用智能体并不会知晓马尔可夫决策所有元素状态转移函数和奖励函数很难估计甚至连环境中的状态都是未知的这就需要免模型强化学习免模型强化学习没有对真实环境进行建模智能体只要在真实环境中通过一定的策略来执行动作等待奖励和状态迁移然后根据这些反馈信息来更新动作策略直至最优有模型和免模型强化学习有什么区别有模型强化学习是根据环境中的经验构建一个虚拟的世界同时在真实环境和虚拟环境中学习免模型强化学习是不对环境进行建模直接与真实环境进行交互来学习到最优策略实际应用中先思考智能体执行动作前是否能对下一步的状态和奖励进行预测如果能就能够对环境进行建模从而采用有模型学习免模型强化学习通常属于数据驱动型方法大量采样来估计状态动作奖励函数从而优化动作策略目前大部分深度强化学习方法都采用了免模型强化学习这是因为免模型强化学习更为简单直观且有丰富的开源资料如系列都采用免模型强化学习在目前的强化学习研究中大部分情况下环境都是静态的可描述的智能体的状态是离散的可观察的如雅达利游戏平台这种相对简单确定的问题并不需要评估状态转移函数和奖励函数可直接采用免模型强化学习使用大量的样本进行训练就能获得较好的效果学习和规划学习和规划是序列决策的两个基本问题在规划中环境是已知的智能体被告知了整个环境能够计算出一个完美的模型并且不需要与环境进行任何交互就能计算只需要知道当前的状态就开始思考寻找最优解探索和利用在强化学习里面探索和利用是两个很核心的问题探索即我们去探索环境通过尝试不同的动作来得到最佳的策略带来最大奖励的策略探索通过试错来理解采取的动作到底可不可以带来好的奖励在状态中尝试新的动作观察奖励发现有关环境的更多信息但是牺牲奖励游戏示例玩一种更新颖的实验动作利用不去尝试新的动作而是采取已知的可以带来很大奖励的动作利用先前发现的好动作利用已知的信息最大化奖励但牺牲未探索的区域游戏示例玩你认为的最好的动作与监督学习任务不同强化学习任务的最终奖励在多步动作之后才能观察到这里我们不妨先考虑比较简单的情形最大化单步奖励即仅考虑一步动作想要最大化单步奖励考虑两个方面每个动作带来的奖励执行奖励最大的动作实际上单步强化学习任务对应一个理论模型即臂赌博机有个摇臂赌徒在投入一个硬币后可选择按下其中一个摇臂每个摇臂以一定的概率吐出硬币但这个概率赌徒并不知道赌徒的目标是通过一定的策略最大化自己的奖励即获得最多的硬币仅探索法将所有的尝试机会平均分配给每个摇臂即轮流按下每个摇臂最后以每个摇臂各自的平均吐币概率作为其奖励期望的近似估计仅利用法按下目前最优的即到目前为止平均奖励最大的摇臂若有多个摇臂同为最优则从中随机选取一个事实上探索和利用本身就是矛盾的要想获取累计奖励最大要获得较好的折中如何平衡探索和利用贪婪探索算法以非零概率尝试所有可能的动作以概率随机选择一个动作探索以概率选择最佳动作利用贪婪动作定义为迄今为止发现的最佳动作最初很大并随着时间推移逐渐退火减小强化学习实验平台的库是一个环境仿真库里面包含很多现有的环境针对不同的场景我们可以选择不同的环境离散控制场景输出的动作是可数的比如游戏中输出的向上或向下动作一般使用雅达利环境评估连续控制场景输出的动作是不可数的比如机器人走路时不仅有方向还有角度角度就是不可数的是一个连续的量一般使用环境评估是对环境的进一步扩展包含更多的游戏关键词强化学习智能体可以在与复杂且不确定的环境进行交互时尝试使所获得的奖励最大化的算法动作环境接收到的智能体基于当前状态的输出状态智能体从环境中获取的状态奖励智能体从环境中获取的反馈信号这个信号指定了智能体在某一步采取了某个策略以后是否得到奖励以及奖励的大小探索在当前的情况下继续尝试新的动作其有可能得到更高的奖励也有可能一无所有利用在当前的情况下继续尝试已知的可以获得最大奖励的过程即选择重复执行当前动作深度强化学习不需要手动设计特征仅需要输入状态就可以让系统直接输出动作的一个端到端的强化学习方法通常使用神经网络来拟合价值函数或者策略网络全部可观测完全可观测和部分可观测当智能体的状态与环境的状态等价时我们就称这个环境是全部可观测的当智能体能够观察到环境的所有状态时我们称这个环境是完全可观测的一般智能体不能观察到环境的所有状态时我们称这个环境是部分可观测的部分可观测马尔可夫决策过程即马尔可夫决策过程的泛化部分可观测马尔可夫决策过程依然具有马尔可夫性质但是其假设智能体无法感知环境的状态只能知道部分观测值动作空间离散动作空间和连续动作空间在给定的环境中有效动作的集合被称为动作空间智能体的动作数量有限的动作空间称为离散动作空间反之则被称为连续动作空间基于策略的智能体会制定一套动作策略即确定在给定状态下需要采取何种动作并根据这个策略进行操作强化学习算法直接对策略进行优化使制定的策略能够获得最大的奖励基于价值的智能体不需要制定显式的策略它维护一个价值表格或者价值函数并通过这个价值表格或价值函数来执行使得价值最大化的动作有模型结构智能体通过学习状态的转移来进行决策免模型结构智能体没有直接估计状态的转移也没有得到环境的具体转移变量它通过学习价值函数或者策略网络进行决策马尔可夫决策过程马尔可夫过程马尔可夫性质马尔可夫性质是指一个随机过程在给定现在状态过去所有状态情况下其未来状态的条件概率分布仅依赖于当前状态以离散随机过程为例假设随机变量构成一个随机过程随机变量所有取值可能为状态空间如果对于过去的状态的条件概率分布仅仅是的一个函数则可以简单的理解为未来的状态只与当前的状态有关而与过去的状态无关马尔可夫性质是所有马尔可夫过程的基础马尔可夫链马尔可夫过程是一组具有马尔可夫性质的随机变量序列其中下一时刻的状态只取决于当前的状态我们假设历史状态为包含之前所有的状态马尔可夫过程满足从当前状态转移到等效于从之前所有状态转移到离散时间的马尔可夫过程叫做马尔可夫链是最简单的马尔可夫过程状态是有限的可以认为每个状态到下一个状态的状态转移过程是不同概率的我们可以用状态转移矩阵来描述状态转移状态转移矩阵描述的每一行是从一个状态到其他所有状态的概率马尔可夫奖励过程马尔可夫奖励过程是马尔可夫链奖励函数在马尔可夫将过程中状态转移矩阵和状态都与马尔可夫链一样只是多了奖励函数奖励函数是一个期望表示当我们到达某个状态可以获得多大的奖励这里另外定义了折扣因子如果状态数量是有限的那么可以是一个向量回报与价值函数范围是指一个回合的长度每个回合最大是时间步数它是由有限个步数决定的回报定义为奖励的逐步叠加假设时刻后的奖励序列为则回报为回报是对未来而言的是最终时刻是折扣因子越往后得到的奖励折扣就越多说明我们更希望得到现有的奖励对未来的奖励要打折扣当有了回报之后我们就可以定义状态的价值就是状态价值函数对应马尔可夫奖励过程状态价值函数被定义为回报的期望其中是之前定义的折扣回报我们对定义了一个期望期望就是从这个状态开始我们可能获得多大的价值所以期望可以看成未来可能获得的奖励的当前价值的表现简而言之就是当我们进入一个状态时我们现在由多大的价值这个价值的考量看的是未来的回报为什么使用折扣因子马尔可夫过程有的带环并不会终结避免无穷的奖励并不能建立完美的模拟环境的模型对未来的评估不一定是准确的不能完全信任模型因为这种不确定性因此要对未来的评估加一个折扣如果奖励有实际价值我跟更希望得到及时的奖励而不是后面再得到奖励现在的钱比以后的钱更有价值折扣因子可以认为是强化学习智能体的一个超参数如何计算回报用向量表示进入某个状态得到的奖励从出发得到四步之内的轨迹分别计算折扣回报如何计算价值函数一个可行的做法是针对一个状态生成所有的轨迹然后把轨迹回报都计算出来然后取平均值作为进入状态的价值这种叫做蒙特卡洛采样方法贝尔曼方程这里采用另外一种方法计算价值函数从价值函数里推导贝尔曼方程即时类励未来奖励的折扣总和可以看成未来的某个状态是指当前状态转移到未来状态的概率代表的是未来某一个状态的价值我们从当前状态开始有一定概率去到未来所有的状态得到未来状态的价值和前往未来状态的概率求和乘以折扣因子可以堪称对未来奖励的折扣贝尔曼方程定义了当前状态和未来状态之间的关系未来奖励的折扣总和加上即时的奖励就组成了贝尔曼方程全期望公式全期望公式也被称为期望折叠公式如果是样本空间的有限或可数的划分则全期望公式可以被定义为现在证明贝尔曼方程的推导马尔可夫决策过程',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-06 15:08:10',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://index.anheyu.com/" title="个人主页"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/13/64d8c2748ef34.jpg" alt="个人主页"/><span class="back-menu-item-text">个人主页</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/07/23/64bc72c75319d.png" alt="博客"/><span class="back-menu-item-text">博客</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/13/64d8c2653332e.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/13/64d8c2653332e.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">11的Blog</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button only-home" id="travellings_button" title="随机前往一个开往项目网站"><a class="site-page" onclick="anzhiyu.totraveling()" title="随机前往一个开往项目网站" href="javascript:void(0);" rel="external nofollow" data-pjax-state="external"><i class="anzhiyufont anzhiyu-icon-train"></i></a></div><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="/img/wechatpay.jpg" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/wechatpay.jpg"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/alipay.jpg"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/408/" style="font-size: 1.05rem;">408<sup>4</sup></a><a href="/tags/Anaconda/" style="font-size: 1.05rem;">Anaconda<sup>1</sup></a><a href="/tags/C/" style="font-size: 1.05rem;">C++<sup>1</sup></a><a href="/tags/CMake/" style="font-size: 1.05rem;">CMake<sup>1</sup></a><a href="/tags/Carla/" style="font-size: 1.05rem;">Carla<sup>1</sup></a><a href="/tags/Docker/" style="font-size: 1.05rem;">Docker<sup>1</sup></a><a href="/tags/GNSS/" style="font-size: 1.05rem;">GNSS<sup>1</sup></a><a href="/tags/Git/" style="font-size: 1.05rem;">Git<sup>1</sup></a><a href="/tags/Latex/" style="font-size: 1.05rem;">Latex<sup>1</sup></a><a href="/tags/Linux/" style="font-size: 1.05rem;">Linux<sup>1</sup></a><a href="/tags/Pipe/" style="font-size: 1.05rem;">Pipe<sup>1</sup></a><a href="/tags/Python/" style="font-size: 1.05rem;">Python<sup>1</sup></a><a href="/tags/Ros/" style="font-size: 1.05rem;">Ros<sup>1</sup></a><a href="/tags/SCP/" style="font-size: 1.05rem;">SCP<sup>1</sup></a><a href="/tags/SSH/" style="font-size: 1.05rem;">SSH<sup>1</sup></a><a href="/tags/Tmux/" style="font-size: 1.05rem;">Tmux<sup>2</sup></a><a href="/tags/Vim/" style="font-size: 1.05rem;">Vim<sup>2</sup></a><a href="/tags/WSL/" style="font-size: 1.05rem;">WSL<sup>1</sup></a><a href="/tags/easyRL%E8%98%91%E8%8F%87%E4%B9%A6/" style="font-size: 1.05rem;">easyRL蘑菇书<sup>1</sup></a><a href="/tags/%E5%9B%BE%E5%BA%8A/" style="font-size: 1.05rem;">图床<sup>1</sup></a><a href="/tags/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/" style="font-size: 1.05rem;">实用教程<sup>4</sup></a><a href="/tags/%E5%AF%BC%E8%88%AA%E5%AE%9A%E4%BD%8D/" style="font-size: 1.05rem;">导航定位<sup>1</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 1.05rem;">工具<sup>1</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/" style="font-size: 1.05rem;">工具配置<sup>1</sup></a><a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">强化学习<sup>1</sup></a><a href="/tags/%E6%83%AF%E6%80%A7%E5%AF%BC%E8%88%AA/" style="font-size: 1.05rem;">惯性导航<sup>1</sup></a><a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 1.05rem;">操作系统<sup>2</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 1.05rem;">数据结构<sup>1</sup></a><a href="/tags/%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%B7%A5%E5%85%B7/" style="font-size: 1.05rem;">文本编辑工具<sup>2</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 1.05rem;">机器人操作系统<sup>1</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7/" style="font-size: 1.05rem;">深度学习工具<sup>5</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.05rem;">算法<sup>1</sup></a><a href="/tags/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5/" style="font-size: 1.05rem;">组会汇报<sup>2</sup></a><a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" style="font-size: 1.05rem;">编程语言<sup>1</sup></a><a href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" style="font-size: 1.05rem;">自动驾驶<sup>4</sup></a><a href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E4%BB%BF%E7%9C%9F/" style="font-size: 1.05rem;">自动驾驶仿真<sup>1</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/" style="font-size: 1.05rem;">计算机组成原理<sup>1</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" style="font-size: 1.05rem;">计算机网络<sup>1</sup></a><a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 1.05rem;">论文<sup>1</sup></a><a href="/tags/%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92/" style="font-size: 1.05rem;">路径规划<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">5</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">4</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/04/"><span class="card-archive-list-date">四月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">三月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">一月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">九月 2023</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div><div class="console-btn-item" id="consoleKeyboard" onclick="anzhiyu.keyboardToggle()" title="快捷键开关"><a class="keyboard-switch"><i class="anzhiyufont anzhiyu-icon-keyboard"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" itemprop="url">自动驾驶</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>自动驾驶</span></a><a class="article-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>强化学习</span></a><a class="article-meta__tags" href="/tags/easyRL%E8%98%91%E8%8F%87%E4%B9%A6/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>easyRL蘑菇书</span></a></span></div></div><h1 class="post-title" itemprop="name headline">强化学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2024-08-01T04:02:03.000Z" title="发表于 2024-08-01 12:02:03">2024-08-01</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2024-10-06T07:08:10.904Z" title="更新于 2024-10-06 15:08:10">2024-10-06</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">9k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator"></span><span class="post-meta-pv-cv" id="" data-flag-title="强化学习"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="busuanzi_value_page_pv"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为江苏/徐州"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>江苏/徐州</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/d7b8a1da4176723d2a85993fae53b10a37d7ea4f.png"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://11andyy.github.io/2024/08/01/AutonomousDriving/easyRL/"><header><a class="post-meta-categories" href="/categories/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" itemprop="url">自动驾驶</a><a href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" tabindex="-1" itemprop="url">自动驾驶</a><a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" tabindex="-1" itemprop="url">强化学习</a><a href="/tags/easyRL%E8%98%91%E8%8F%87%E4%B9%A6/" tabindex="-1" itemprop="url">easyRL蘑菇书</a><h1 id="CrawlerTitle" itemprop="name headline">强化学习</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">11andyy</span><time itemprop="dateCreated datePublished" datetime="2024-08-01T04:02:03.000Z" title="发表于 2024-08-01 12:02:03">2024-08-01</time><time itemprop="dateCreated datePublished" datetime="2024-10-06T07:08:10.904Z" title="更新于 2024-10-06 15:08:10">2024-10-06</time></header><h1 id="强化学习基础"><a href="#强化学习基础" class="headerlink" title="强化学习基础"></a>强化学习基础</h1><p>到目前为止：</p>
<ol>
<li>学习，需要大量的专家演示</li>
<li>使用辅助短期损失函数</li>
<li>模仿学习：动作的每帧损失</li>
<li>直接感知：可供性指标的每帧损失</li>
</ol>
<p>现在：</p>
<p>​    基于我们真正关心的损失模型</p>
<ol>
<li>最小化到达目标的时间</li>
<li>最小化碰撞次数</li>
<li>最小化风险</li>
</ol>
<p>学习的类型：</p>
<ol>
<li>监督学习<ol>
<li>数据集：<code>&#123;(xi,yi)&#125;(xi=数据,yi=标签)</code>，目标：学习映射<code>x -&gt; y</code></li>
<li>示例：分类、回归、模仿学习、可供性学习</li>
</ol>
</li>
<li>无监督学习<ol>
<li>数据集：<code>&#123;(xi)&#125;(xi=数据)</code>，目标：发现数据的底层结构</li>
<li>示例：聚类、降维、特征学习</li>
</ol>
</li>
<li>强化学习<ol>
<li>代理与提供数字奖励信号的环境交互</li>
<li>目标：学习如何采取行动以最大化奖励</li>
<li>示例：学习操纵或控制任务（所有交互）</li>
</ol>
</li>
</ol>
<h2 id="强化学习概述"><a href="#强化学习概述" class="headerlink" title="强化学习概述"></a>强化学习概述</h2><p>强化学习（Reinforcement，RL）讨论的问题是<strong>智能体（agent）怎么在复杂的、不确定的环境（environment）中最大化它能获得的奖励</strong>。</p>
<p>强化学习由两部分组成，智能体和环境，在强化学习过程中，智能体和环境一直交互：智能体在环境中获取某个状态（state）后，利用该状态输出一个动作（action），这个动作也称为决策（decision），然后该动作在环境中被执行，环境根据智能体采取的动作，输出<strong>下一个状态</strong>和<strong>当前动作带来的奖励</strong>。</p>
<p>智能体的目的就是尽可能多地从环境中获取奖励。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240801104547830.png" alt="image-20240801104547830"></p>
<ol>
<li>agent在时间$t$内观察环境状态$S_t$</li>
<li>在时间$t$结束对环境的操作$A_t$</li>
<li>环境将奖励$R_{t+1}$和新状态$S_{t+1}$返回给agent</li>
</ol>
<ol>
<li>目标：选择动作以最大化奖励</li>
<li>行动可能会产生长期影响</li>
<li>回报可能是延迟的而不是及时的，牺牲及时回报获得长期回报可能更好</li>
</ol>
<h3 id="强化学习与监督学习"><a href="#强化学习与监督学习" class="headerlink" title="强化学习与监督学习"></a>强化学习与监督学习</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240801115047123.png" alt="image-20240801115043758"></p>
<p>以图片分类为例，<strong>监督学习（supervised learning）</strong>假设我们有大量被标注的数据，这些图片独立同分布，之间没有关联关系。训练一个分类器，在训练过程中，需要正确的的标签传给圣经网络，当做出错误预测时会告诉网络预测错误。根据类似错误写出一个损失函数（loss），通过反向传播来训练神经网络</p>
<p>在监督学习中，有两个阶段：</p>
<ul>
<li>输入的数据之间是没有关联的</li>
<li>需要告诉学习器正确的标签是什么</li>
</ul>
<p>在强化学习中，监督学习的两个假设都不成立，以雅达利游戏（Atari）为例（打砖块），智能体游戏过程中得到的观测不是独立同分布的，上一帧和下一帧有非常强的连续性，得到的数据是<strong>相关的时间序列数据</strong>，另外，我们没有得到立刻反馈，游戏并没有告诉我们哪个动作是正确的。</p>
<p>因此强化学习的困难之处便是智能体不能得到及时的反馈，但是我们依然希望智能体能在这个环境中学习。</p>
<p><strong>因此，强化学习的训练数据是一个玩游戏的过程</strong>，从第一步开始，采取一个动作，第二步又采取一个动作，得到的是一个玩游戏的序列，假设当前是第三步，<strong>我们把当前的序列放进网络，希望网络可以得到一个输出动作</strong>。但是我们没有标签说明输出的动作是正确or错误，必须等游戏结束才会知道，这就面临着<strong>延迟奖励</strong>问题</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240801151440893.png" alt="image-20240801151440893"></p>
<p><strong>强化学习和监督学习的区别：</strong></p>
<ol>
<li>强化学习输入的样本是序列数据，而不像监督学习中样本是独立的</li>
<li>学习器并没有告诉每一步正确的动作是什么，学习器需要自己去发现那些动作可以带来更多的奖励，只有通过不断地尝试才能找到最有利的动作</li>
<li>智能体获得自己能力的过程，其实是不断试错探索的过程。<strong>探索和利用</strong>是强化学习中的核心问题。探索是尝试一些新的动作，这些新的动作可能使我们得到更多地奖励，也可能一无所有；利用是采取已知的可以获得最多奖励的动作，重复执行这个动作。因此我们需要<strong>在探索和利用之间权衡</strong></li>
<li>在强化学习中没有非常强的监督者，只有<strong>奖励信号</strong>，并且<strong>奖励信号是延迟的</strong>，即环境在很久以后才会告诉我们奖励是不是有效的</li>
</ol>
<p><strong>强化学习的特点：</strong></p>
<ol>
<li>强化学习会试错探索，通过探索环境来获取对环境的理解</li>
<li>强化学习智能体会从环境里面获得延迟的奖励</li>
<li>在强化学习的训练过程中，时间非常重要，<strong>因为我们获得的是有时间关联的数据，而不是独立同分布的数据</strong></li>
<li>智能体的动作会影响它随后的到数据，在强化学习中一个很重要的问题是如何让智能体的动作稳定提升</li>
</ol>
<h3 id="强化学习的应用"><a href="#强化学习的应用" class="headerlink" title="强化学习的应用"></a>强化学习的应用</h3><ol>
<li>走路的智能体</li>
<li>机械臂抓取、魔方</li>
<li>Alpha Go</li>
<li>自动驾驶</li>
</ol>
<h2 id="序列决策"><a href="#序列决策" class="headerlink" title="序列决策"></a>序列决策</h2><h3 id="智能体与环境"><a href="#智能体与环境" class="headerlink" title="智能体与环境"></a>智能体与环境</h3><p>强化学习研究的问题实际上就是智能体和环境交互的问题，<strong>智能体把动作输出到环境，环境取得这个动作后进入下一步，并把下一步的观测结果返回给智能体</strong>，这样的交互会产生大量的观测，智能体的目的就是<strong>从这些观测中学到最大化奖励的策略</strong>。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240801153620417.png" alt="image-20240801153620417"></p>
<h3 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h3><p>奖励是由环境给的一种标量的反馈信号，这种信号显示<strong>智能体在某一步骤采取某个策略的表现如何</strong>，在不同的环境中，奖励的标准也是不同的（评判依据不一样）</p>
<h3 id="序列决策-1"><a href="#序列决策-1" class="headerlink" title="序列决策"></a>序列决策</h3><p>在一个强化学习环境中，智能体的目的就是选取一系列的动作来最大化奖励，所以我们选取的动作必须有长期的影响，但是在这个过程中，智能体的奖励实际上是被延迟的。在强化学习中，一个重要的课题就是<strong>近期奖励</strong>和<strong>远期奖励</strong>的权衡，研究如何让智能体获得更多的远期奖励。</p>
<p>在与环境交互的过程中，智能体会获得很多观测，针对每一个观测，智能体会采取一个动作，也会得到一个奖励，所以历史是<strong>观测、动作、奖励</strong>的序列：</p>
<script type="math/tex; mode=display">
H_t = o_1,\alpha_1,r_1,...,o_t,\alpha_t,r_t</script><p>智能体在采取当前动作的时候会依赖与它之前得到的历史，所以我们可以把整个游戏状态看成关于这个历史的函数：</p>
<script type="math/tex; mode=display">
S_t = f(H_t)</script><blockquote>
<p>Q：状态和观测有什么关系？</p>
<p>A：<strong>状态</strong>是对世界的完整描述，不会隐藏世界信息。<strong>观测</strong>是对状态的部分描述，可能会遗漏一些信息，在深度强化学习中，我们几乎总用实值的向量、矩阵或更高阶的张量来表示状态和观测。例如我们可以用RGB像素值矩阵来表示一个视觉的观测，用机器人关节角度和速度来表示一个机器人关节的状态。</p>
</blockquote>
<p>环境有自己的函数$s_t^e=f^e(H_t)$来更新状态；</p>
<p>智能体自己的函数$s_t^a=f^a(H_t)$来更新状态；</p>
<p><strong>重要：</strong>当智能体的状态与环境状态等价时，即当<strong>智能体能够观察到环境所有状态</strong>时，我们称这个环境是<strong>完全可观测的</strong></p>
<p>在这种情况下，强化学习通常被建模成一个<strong>马尔可夫决策过程（MDP）问题</strong>，在马尔可夫决策过程中：</p>
<script type="math/tex; mode=display">
o_t=s_t^e=s_t^{a}</script><p><strong>上述公式表述为t时刻智能体的所有观测=t时刻环境所有状态=t时刻智能体更新得到的所有状态</strong></p>
<p>但是有一种情况，智能体得到的观测并不能包含环境运作的所有状态，因为在强化学习的设定里面，环境状态才会包含所有状态。当智能体只能看到部分观测时，就称这个环境是部分可观测的。强化学习通常被建模为一个<strong>部分可观测的马尔可夫决策过程（POMDP）</strong></p>
<p>POMDP是MDP的泛化，部分可观测的马尔可夫决策过程依然具有马尔可夫的性质，但是假设智能体无法感知环境的状态，只能知道部分观测值。比如在自动驾驶任务中，智能体只能感知传感器采集的<strong>有限环境信息</strong>。部分可观测马尔可夫决策过程可以用一个七元组表示：</p>
<script type="math/tex; mode=display">
(S,A,T,R,\Omega,O,\gamma)</script><p>$S$：状态空间，为隐变量</p>
<p>$A$：动作空间</p>
<p>$T(s’|s,a)$：状态转移概率</p>
<p>$R$：奖励函数</p>
<p>$\Omega(o|s,a)$：观测概率</p>
<p>$O$：观测空间</p>
<p>$\gamma$：折扣系数</p>
<h2 id="动作空间"><a href="#动作空间" class="headerlink" title="动作空间"></a>动作空间</h2><p>不同环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间。像围棋这种离散的动作空间中，智能体的动作数量是有限的；但是在连续的动作空间中，动作是实值的向量。</p>
<p>例如：走迷宫机器人如果只有往东、往西、往南、往北这四种移动方式，则动作空间为<strong>离散动作空间</strong></p>
<p>如果机器人可以360任意角度移动，则动作空间为<strong>连续动作空间</strong>。</p>
<h2 id="强化学习智能体的组成成分和类型"><a href="#强化学习智能体的组成成分和类型" class="headerlink" title="强化学习智能体的组成成分和类型"></a>强化学习智能体的组成成分和类型</h2><ul>
<li>策略（policy）：智能体会用策略选取下一步的动作</li>
<li>价值函数（value function）：用价值函数对当前状态进行评估。价值函数用于评估智能体进入某个状态后可以对后面的奖励带来多大的影响。价值函数越大，说明智能体进入这个状态越有利。</li>
<li>模型（model）：模型表示智能体对环境状态进行理解，决定了环境中世界的运行方式</li>
</ul>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><blockquote>
<p>我的理解：Policy就是当Agent观察到状态后，在该状态应该采取什么样的动作的函数，也就是说<strong>策略是一个从s到a的函数</strong></p>
<script type="math/tex; mode=display">
\pi(a|s)=p(a_t=a|s_t=s)</script><p>$a$是采取的动作，这个动作属于动作空间；只要有了这个函数，输入一个状态，就会采取随机抽样来选取下一个动作。</p>
<p>这个$\pi$函数实际上是一个<strong>概率密度函数</strong>。<strong>表示在$s$状态下，采取$a$动作的可能性。</strong></p>
</blockquote>
<p>策略是智能体的动作模型，决定了智能体的动作。</p>
<p>它其实是一个函数，用于把输入的<strong>状态</strong>变为<strong>动作</strong>。</p>
<p>策略分为两种：随机性策略和决定性策略</p>
<p><strong>随机性策略（stochastic policy）</strong>：就是$\pi$函数，即$\pi(a|s)=p(a_t=a|s_t=s)$。输入一个状态$s$，输出一个概率，这个概率是智能体所有动作的概率，然后对这个概率分布进行采样，可得到智能体将要采取的动作。比如0.7概率往左，0.3概率往右，通过采样就可以得到智能体将要采取的策略。</p>
<p><strong>确定性策略（deterministic policy）</strong>：就是智能体直接采取最有可能的动作，即$a^*=arg\ max \pi(a|s)$</p>
<ol>
<li>MDP策略仅依赖当前的状态，而不是整个历史</li>
<li>但是，当前的状态可能包括过去的观察结果</li>
</ol>
<p><strong>如何学习策略？</strong></p>
<ol>
<li>模仿学习：从专家演示中学习策略<ol>
<li>提供专家演示</li>
<li>监督学习问题强化学习：通过反复试验学习策略</li>
<li>没有专家演示</li>
<li>代理自己发现那些动作可以最大化预期</li>
</ol>
</li>
<li>未来奖励<ol>
<li>代理与环境交互并获得奖励</li>
<li>代理发现更好的动作并改进策略$\pi(a|s)$</li>
</ol>
</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240802111532782.png" alt="image-20240802111532782"></p>
<p>从雅达利游戏来看，策略函数的输入就是游戏的一帧，他的输出能够决定智能体向左或向右移动。</p>
<p>通常情况下，智能体使用随机性策略，在学习中引入一定的随机性可以更好地探索环境，随机性策略的动作具有多样性，这一点在智能体博弈时非常最重要，如果采用确定性策略，很容易被博弈的对手猜到。</p>
<h3 id="价值函数"><a href="#价值函数" class="headerlink" title="价值函数"></a>价值函数</h3><p><strong>贝尔曼最优性（取决于状态）和Q学习（取决于状态+动作）</strong></p>
<blockquote>
<p>这部分的公式不是很理解！</p>
</blockquote>
<p>价值函数是对未来奖励的预测，用来评估状态的好坏。价值函数里有一个<strong>折扣因子</strong>，我们希望可以在尽可能短的时间里得到尽可能多的奖励。</p>
<p>比如现在我们有两个选择：10天后给我们100块，和现在立刻给100块，必然选择后者，因为有利息，因此我们可以把折扣因子放在价值函数中，<strong>状态价值函数</strong>定义为：</p>
<script type="math/tex; mode=display">
V_\pi(s)\doteq\mathbb{E}_\pi\left[G_t\mid s_t=s\right]=\mathbb{E}_\pi\left[\sum_{k=0}^\infty\gamma^kr_{t+k+1}\mid s_t=s\right],\text{对于所有的}s\in S</script><p><strong>期望$\mathbb{E}_\pi$的下标是$\pi$函数，期望的值可以反应我们在状态s下使用策略$\pi$时，到底可以获得多少奖励</strong></p>
<p>关于折扣因子：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240827203001013.png" alt="image-20240827203001013"></p>
<p><strong>当折扣因子小于1时，即时的奖励权重要大于长远的奖励</strong></p>
<p>还有一种定义的价值函数：Q函数，Q函数包括两个变量，状态和动作，定义为：</p>
<script type="math/tex; mode=display">
Q_\pi(s,a)\doteq\mathbb{E}_\pi\left[G_t\mid s_t=s,a_t=a\right]=\mathbb{E}_\pi\left[\sum_{k=0}^\infty\gamma^kr_{t+k+1}\mid s_t=s,a_t=a\right]</script><p>所以未来可以获得奖励的期望取决于<strong>当前状态和当前的动作</strong>。Q函数是强化学习中要学习的一个函数，因为当我们得到Q函数后，进入某个状态要采取的最优动作可以通过Q函数得到</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>模型决定了下一步的状态，下一步的状态取决于当前的状态以及当前采取的动作，它由<strong>状态转移概率和奖励函数</strong>两个部分组成，<strong>状态转移概率</strong>为：</p>
<script type="math/tex; mode=display">
p_{ss'}^a=p\left(s_{t+1}=s'\mid s_t=s,a_t=a\right) \ \ \ 状态转移概率</script><p><strong>表示从状态$s$采取动作$a$转移到状态$s’$的概率。</strong></p>
<p><strong>奖励函数</strong>是指我们在当前状态采取了某个动作，得以得到多大的奖励：</p>
<script type="math/tex; mode=display">
R(s,a)=\mathbb{E}\left[r_{t+1}\mid s_t=s,a_t=a\right] \ \ \ 奖励函数</script><p>当我们友链策略、价值函数、模型三个组成成分后，就形成了一个<strong>马尔可夫决策过程</strong>，这个过程可视化了状态之间的转移以及采取的动作：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240802134032098.png" alt="image-20240802134032098"></p>
<p>例子：走迷宫，要求智能体从起点（start）开始，然后到终点（goal）的位置。每走一步，就会得到-1的奖励。我们可以采取的动作是往上、往下、往左、往右的动作。我们用现在智能体所在的位置来描述当前的状态。</p>
<p>我们可以用不同的强化学习方法来解决这个环境，如果我们采取<strong>基于策略的强化学习方法（policy-based RL）</strong>，当<strong>学习好了这个环境后</strong>，在每一个状态，我们都会得到一个最佳的动作，比如在第一个格子，最佳动作是往右走，通过最佳的策略，我们可以最快地到达终点。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240802134731744.png" alt="image-20240802134731744"></p>
<p>如果换成<strong>基于价值的强化学习方法（value-based RL）</strong>，利用价值函数作为导向，我们就会的得到另外一种表征，在每个状态返回一个价值。比如我们在起点的价值是-16，因为没走一步会-1，最快16步到达终点。</p>
<p>当我们快接近终点时，这个数字变得越来越大。在拐角时，比如现在是-15，智能体会看上下两个格子，上面的价值变大了为，-14，下面的价值是-16，因此它会选取往上走的策略，通过学习价值的不同，我们可以抽象出最佳的策略。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240802135204689.png" alt="image-20240802135204689"></p>
<h3 id="强化学习智能体的类型"><a href="#强化学习智能体的类型" class="headerlink" title="强化学习智能体的类型"></a>强化学习智能体的类型</h3><h4 id="基于价值的智能体与基于策略的智能体"><a href="#基于价值的智能体与基于策略的智能体" class="headerlink" title="基于价值的智能体与基于策略的智能体"></a>基于价值的智能体与基于策略的智能体</h4><ul>
<li>基于价值的智能体：显示地学习价值函数，隐式地学习策略。策略是从价值函数中推算出来的</li>
<li>基于策略的智能体：直接学习策略，我们给它一个状态，他会输出对应动作的概率。基于策略的智能体并没有学习价值函数。</li>
<li>演员-评论员智能体：把两者结合起来，把策略和价值函数都学习了，通过两者交互得到最佳的动作</li>
</ul>
<blockquote>
<p>Q：基于策略和基于价值的强化学习方法有什么区别？</p>
<p>A：对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用<strong>动态规划算法</strong>来求解。</p>
<p>决策方式是只能个体在给定状态下从动作集合中选择一个动作的依据，它是<strong>静态</strong>的，不随状态变化而变化。</p>
<p>在基于策略的强化学习方式中，智能体会制定一套动作策略，并根据这个策略进行操作，强化学习算法直接对这个策略进行优化，使得定制的测录能够获得最大奖励。</p>
<p>在基于价值的强化学习方式中，智能体不需要制定显示的策略，而是维护一个<strong>价值表格或价值函数</strong>，并通过这个价值表格或价值函数来选取价值最大的动作。</p>
<p>基于价值迭代的方法只能应用在不连续的、离散的环境中，比如围棋等。</p>
<p>基于策略迭代的方法可以选择连续的动作，比如动作集合规模庞大、动作连续的场景，机器人控制领域。</p>
<p>基于价值的强化学习算法由Q学习、Sarsa等；基于策略的强化学习方法有策略梯度算法PG，此外演员-评论员算法同时使用策略和价值评估做出决策，其中，智能体根据<strong>策略做出决策，价值函数会对做出的动作给出价值</strong>，这样可以在原有的策略梯度算法基础上加速学习过程，取得更好的效果。</p>
</blockquote>
<h4 id="有模型强化学习智能体与免模型强化学习智能体"><a href="#有模型强化学习智能体与免模型强化学习智能体" class="headerlink" title="有模型强化学习智能体与免模型强化学习智能体"></a>有模型强化学习智能体与免模型强化学习智能体</h4><ul>
<li>有模型强化学习：通过学习状态的转移才采取动作</li>
<li>免模型强化学习：智能体没有直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数和策略函数进行决策，免模型强化学习智能体的模型里面没有<strong>环境转移的模型</strong></li>
</ul>
<p>我们可以用<strong>马尔可夫决策</strong>过程来定义强化学习任务，并将其表示为四元组$(S,A,P,R)$即状态集合、动作集合、状态转移函数和奖励函数。如果这个四元组中的元素均已知，且状态集合和动作集合在有限步数内是有限集，则智能体可以根据真实环境进行建模，构建一个虚拟的世界来模拟真实环境中的状态和交互反应。具体来说，当智能体知道状态转移函数$P(s_{t+1}|s_t,a_t)$和奖励函数$R(s_t,a_t)$后，它就能知道在某一状态下执行某一动作后能带来的奖励和下一状态，直接在虚拟世界学习规划策略</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240802144740795.png" alt="image-20240802144740795"></p>
<p>然而在实际应用，智能体并不会知晓马尔可夫决策所有元素，状态转移函数和奖励函数很难估计，甚至连环境中的状态都是未知的，这就需要免模型强化学习。免模型强化学习没有对真实环境进行建模，智能体只要在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新动作策略直至最优</p>
<blockquote>
<p>Q：有模型和免模型强化学习有什么区别？<br>A：有模型强化学习是根据环境中的经验，构建一个虚拟的世界，同时在真实环境和虚拟环境中学习。</p>
<p>免模型强化学习是不对环境进行建模，直接与真实环境进行交互来学习到最优策略。</p>
<p>实际应用中，先思考智能体执行动作前，是否能对下一步的状态和奖励进行预测，如果能，就能够对环境进行建模，从而采用有模型学习。</p>
<p>免模型强化学习通常属于数据驱动型方法，大量采样来估计状态、动作、奖励函数从而优化动作策略。</p>
<p>目前，大部分深度强化学习方法都采用了免模型强化学习，这是因为：免模型强化学习更为简单、直观且有丰富的开源资料，如 AlphaGo  系列都采用免模型强化学习；在目前的强化学习研究中，大部分情况下环境都是静态的、可描述的，智能体的状态是离散的、可观察的（如雅达利游戏平台），这种相对简单、确定的问题并不需要评估状态转移函数和奖励函数，可直接采用免模型强化学习，使用大量的样本进行训练就能获得较好的效果。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240802145453849.png" alt="image-20240802145453849"></p>
<h2 id="学习和规划"><a href="#学习和规划" class="headerlink" title="学习和规划"></a>学习和规划</h2><p>学习和规划是序列决策的两个基本问题</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240802145532597.png" alt="image-20240802145532597"></p>
<p>在规划中，环境是已知的，智能体被告知了整个环境，能够计算出一个完美的模型，并且不需要与环境进行任何交互就能计算，只需要知道当前的状态，就开始思考，寻找最优解</p>
<h2 id="探索和利用"><a href="#探索和利用" class="headerlink" title="探索和利用"></a>探索和利用</h2><p>在强化学习里面，<strong>探索</strong>和<strong>利用</strong>是两个很核心的问题。 探索即我们去探索环境，通过尝试不同的动作来得到最佳的策略（带来最大奖励的策略）</p>
<p><strong>探索：</strong>通过试错来理解采取的动作到底可不可以带来好的奖励，在状态$s_t$中尝试新的动作$a_t$，观察奖励$s_{t+1}$</p>
<ol>
<li>发现有关环境的更多信息，但是牺牲奖励</li>
<li>游戏示例：<strong>玩一种更新颖的实验动作</strong></li>
</ol>
<p><strong>利用：</strong>不去尝试新的动作，而是采取已知的可以带来很大奖励的动作，利用先前发现的好动作$a$</p>
<ol>
<li>利用已知的信息最大化奖励，但牺牲未探索的区域</li>
<li>游戏示例：<strong>玩你认为的最好的动作</strong></li>
</ol>
<p>与监督学习任务不同，强化学习任务的最终奖励在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：最大化单步奖励，即仅考虑一步动作。</p>
<p>想要最大化单步奖励考虑两个方面：</p>
<ol>
<li>每个动作带来的奖励</li>
<li>执行奖励最大的动作</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240802185054502.png" alt="image-20240802185054502"></p>
<p>实际上，单步强化学习任务对应一个理论模型，即<strong>K-臂赌博机</strong>，有K个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖励，即获得最多的硬币。</p>
<p><strong>仅探索（exploration-only）法</strong>：将所有的尝试机会平均分配给每个摇臂（即轮流按下每个摇臂），最后以每个摇臂各自的平均吐币概率作为其奖励期望的近似估计。</p>
<p><strong>仅利用（exploitation-only）法</strong>：按下目前最优的（即到目前为止平均奖励最大的）摇臂，若有多个摇臂同为最优，则从中随机选取一个。</p>
<p>事实上探索和利用本身就是矛盾的，要想获取累计奖励最大，要获得较好的折中</p>
<p><strong>如何平衡探索和利用？</strong></p>
<p>$\epsilon $-贪婪探索算法：</p>
<ol>
<li>以非零概率尝试所有可能的动作</li>
<li>以概率$\epsilon $随机选择一个动作（探索）</li>
<li>以概率$1-\epsilon $选择最佳动作（利用）</li>
<li>贪婪动作定义为迄今为止发现的最佳动作</li>
<li>$\epsilon$最初很大，并随着时间推移逐渐退火（$\epsilon$减小 ）</li>
</ol>
<h2 id="强化学习实验平台"><a href="#强化学习实验平台" class="headerlink" title="强化学习实验平台"></a>强化学习实验平台</h2><h3 id="Gym"><a href="#Gym" class="headerlink" title="Gym"></a>Gym</h3><p>OpenAI 的 <strong>Gym库</strong>是一个环境仿真库，里面包含很多现有的环境。针对不同的场景，我们可以选择不同的环境。离散控制场景（输出的动作是可数的，比如Pong游戏中输出的向上或向下动作）一般使用雅达利环境评估；连续控制场景（输出的动作是不可数的，比如机器人走路时不仅有方向，还有角度，角度就是不可数的，是一个连续的量 ）一般使用 MuJoCo 环境评估。<strong>Gym Retro</strong>是对 Gym 环境的进一步扩展，包含更多的游戏。</p>
<h3 id="MountainCar-v0"><a href="#MountainCar-v0" class="headerlink" title="MountainCar-v0"></a>MountainCar-v0</h3><h2 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h2><ul>
<li><strong>强化学习（reinforcement learning，RL）</strong>：智能体可以在与复杂且不确定的环境进行交互时，尝试使所获得的奖励最大化的算法。</li>
<li><strong>动作（action）</strong>： 环境接收到的智能体基于当前状态的输出。</li>
<li><strong>状态（state）</strong>：智能体从环境中获取的状态。</li>
<li><strong>奖励（reward）</strong>：智能体从环境中获取的反馈信号，这个信号指定了智能体在某一步采取了某个策略以后是否得到奖励，以及奖励的大小。</li>
<li><strong>探索（exploration）</strong>：在当前的情况下，继续尝试新的动作。其有可能得到更高的奖励，也有可能一无所有。</li>
<li><strong>利用（exploitation）</strong>：在当前的情况下，继续尝试已知的可以获得最大奖励的过程，即选择重复执行当前动作。</li>
<li><strong>深度强化学习（deep reinforcement learning）</strong>：不需要手动设计特征，仅需要输入状态就可以让系统直接输出动作的一个端到端（end-to-end）的强化学习方法。通常使用神经网络来拟合价值函数（value function）或者策略网络（policy network）。</li>
<li><strong>全部可观测（full observability）、完全可观测（fully observed）和部分可观测（partially observed）</strong>：当智能体的状态与环境的状态等价时，我们就称这个环境是全部可观测的；当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的；一般智能体不能观察到环境的所有状态时，我们称这个环境是部分可观测的。</li>
<li><strong>部分可观测马尔可夫决策过程（partially observable Markov decision process，POMDP）</strong>：即马尔可夫决策过程的泛化。部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是其假设智能体无法感知环境的状态，只能知道部分观测值。</li>
<li><strong>动作空间（action space）、离散动作空间（discrete action space）和连续动作空间（continuous action space）</strong>：在给定的环境中，有效动作的集合被称为动作空间，智能体的动作数量有限的动作空间称为离散动作空间，反之，则被称为连续动作空间。</li>
<li><strong>基于策略的（policy-based）</strong>：智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。</li>
<li><strong>基于价值的（valued-based）</strong>：智能体不需要制定显式的策略，它维护一个价值表格或者价值函数，并通过这个价值表格或价值函数来执行使得价值最大化的动作。</li>
<li><strong>有模型（model-based）结构</strong>：智能体通过学习状态的转移来进行决策。</li>
<li><strong>免模型（model-free）结构</strong>：智能体没有直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数或者策略网络进行决策。</li>
</ul>
<h1 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h1><h2 id="马尔可夫过程（MP）"><a href="#马尔可夫过程（MP）" class="headerlink" title="马尔可夫过程（MP）"></a>马尔可夫过程（MP）</h2><h3 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h3><p>马尔可夫性质是指一个随机过程在给定<strong>现在状态+过去所有状态</strong>情况下，其<strong>未来状态的条件概率分布仅依赖于当前状态</strong>。</p>
<p>以离散随机过程为例，假设随机变量$X_0,X_1,…,X_T$构成一个随机过程，随机变量所有取值可能为状态空间，如果$X_{t+1}$对于过去的状态的条件概率分布仅仅是$X_t$的一个函数，则：</p>
<script type="math/tex; mode=display">
p\left(X_{t+1}=x_{t+1}\mid X_{0:t}=x_{0:t}\right)=p\left(X_{t+1}=x_{t+1}\mid X_t=x_t\right)</script><p><strong>可以简单的理解为未来的状态只与当前的状态有关，而与过去的状态无关</strong>，马尔可夫性质是所有马尔可夫过程的基础。</p>
<h3 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h3><p>马尔可夫过程是一组具有马尔可夫性质的随机变量序列$s_1,s_2,…,s_t$，其中下一时刻的状态$s_{t+1}$只取决于当前的状态。我们假设历史状态为$h_t=\{s_1,s_2,…,s_t\}$，$h_t$包含之前所有的状态，马尔可夫过程满足：</p>
<script type="math/tex; mode=display">
p(s_{t+1}|s_t)=p(s_{t+1}|h_t)</script><p><strong>从当前状态$s_t$转移到$s_{t+1}$等效于从之前所有状态$h_t$转移到$s_{t+1}$</strong></p>
<p>离散时间的马尔可夫过程叫做<strong>马尔可夫链</strong>，是最简单的马尔可夫过程，状态是有限的</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240828093247503.png" alt="image-20240828093247503"></p>
<p>可以认为每个状态到下一个状态的状态转移过程，是不同概率的。</p>
<p>我们可以用状态转移矩阵$P$来描述状态转移$p(s_{t+1}=s’|s_t=s)$</p>
<script type="math/tex; mode=display">
\left.\boldsymbol{P}=\left(\begin{array}{cccc}p\left(s_1\mid s_1\right)&p\left(s_2\mid s_1\right)&\ldots&p\left(s_N\mid s_1\right)\\p\left(s_1\mid s_2\right)&p\left(s_2\mid s_2\right)&\ldots&p\left(s_N\mid s_2\right)\\\vdots&\vdots&\ddots&\vdots\\p\left(s_1\mid s_N\right)&p\left(s_2\mid s_N\right)&\ldots&p\left(s_N\mid s_N\right)\end{array}\right.\right)</script><p><strong>状态转移矩阵描述的每一行是从一个状态到其他所有状态的概率</strong></p>
<h2 id="马尔可夫奖励过程（MRP）"><a href="#马尔可夫奖励过程（MRP）" class="headerlink" title="马尔可夫奖励过程（MRP）"></a>马尔可夫奖励过程（MRP）</h2><p>马尔可夫奖励过程是<strong>马尔可夫链+奖励函数</strong>，在马尔可夫将i过程中，状态转移矩阵和状态都与马尔可夫链一样，<strong>只是多了奖励函数</strong>。</p>
<p>奖励函数$R$是一个期望，表示当我们到达某个状态，可以获得多大的奖励。</p>
<p>这里另外定义了折扣因子$\gamma $。如果状态数量是有限的，那么$R$可以是一个向量。</p>
<h3 id="回报与价值函数"><a href="#回报与价值函数" class="headerlink" title="回报与价值函数"></a>回报与价值函数</h3><p>范围：是指一个回合的长度（每个回合最大是时间步数），它是由有限个步数决定的</p>
<p>回报：定义为奖励的逐步叠加，假设$t$时刻后的奖励序列为$r_{t+1},r_{t+2},r_{t+3},…,$则回报为：</p>
<script type="math/tex; mode=display">
G_t=r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\gamma^3r_{t+4}+\ldots+\gamma^{T-t-1}r_T</script><p><strong>回报是对未来而言的</strong>，$T$是最终时刻，$\gamma $是折扣因子，<strong>越往后得到的奖励，折扣就越多，说明我们更希望得到现有的奖励，对未来的奖励要打折扣</strong></p>
<p>当有了回报之后，我们就可以定义状态的价值。就是<strong>状态价值函数</strong>，对应马尔可夫奖励过程，状态价值函数被定义为<strong>回报的期望</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
V^{t}(s)& =\mathbb{E}\left[G_{t}\mid s_{t}=s\right] \\
&=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^2r_{t+3}+\ldots+\gamma^{T-t-1}r_T\mid s_t=s\right]
\end{aligned}</script><p>其中$G_t$是之前定义的<strong>折扣回报</strong>，我们对$G_t$定义了一个期望，期望就是从这个状态开始，我们可能获得多大的价值。<strong>所以期望可以看成未来可能获得的奖励的当前价值的表现</strong>，简而言之就是当我们进入一个状态时，我们现在由多大的价值，这个价值的考量看的是未来的回报。</p>
<blockquote>
<p>Q：为什么使用折扣因子？</p>
<ol>
<li>马尔可夫过程有的带环，并不会终结，避免无穷的奖励</li>
<li>并不能建立完美的模拟环境的模型，对未来的评估不一定是准确的，不能完全信任模型，因为这种不确定性，因此要对未来的评估加一个折扣</li>
<li>如果奖励有实际价值，我跟更希望得到及时的奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）</li>
<li>折扣因子可以认为是强化学习智能体的一个超参数</li>
</ol>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240828101226290.png" alt="image-20240828101226290"></p>
<p>如何计算回报？</p>
<p>用向量$R$表示进入某个状态得到的奖励，从$s_4$出发得到四步之内的轨迹，分别计算<strong>折扣回报</strong></p>
<p>如何计算价值函数？</p>
<p>一个可行的做法是针对一个状态，生成所有的轨迹，然后把轨迹回报都计算出来，然后取平均值作为进入$s_4$状态的价值。这种叫做蒙特卡洛采样方法。</p>
<h3 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h3><p>这里采用另外一种方法计算价值函数，从价值函数里推导贝尔曼方程。</p>
<script type="math/tex; mode=display">
V(s)=\underbrace{R(s)}_{\text{即时类励}}+\underbrace{\gamma\sum_{s^{\prime}\in S}p\left(s^{\prime}\mid s\right)V\left(s^{\prime}\right)}_{\text{未来奖励的折扣总和}}</script><p>$s’$可以看成未来的某个状态，$p(s’|s)$是指当前状态转移到未来状态的概率</p>
<p>$V(s’)$代表的是未来某一个状态的价值。</p>
<p>我们从当前状态开始，有一定概率去到未来所有的状态。</p>
<p>得到未来状态的价值和前往未来状态的概率，求和乘以折扣因子，可以堪称对未来奖励的折扣。</p>
<p><strong>贝尔曼方程定义了当前状态和未来状态之间的关系，未来奖励的折扣总和加上即时的奖励，就组成了贝尔曼方程</strong></p>
<h4 id="全期望公式"><a href="#全期望公式" class="headerlink" title="全期望公式"></a>全期望公式</h4><p>全期望公式也被称为期望折叠公式。如果$A_i$是样本空间的有限或可数的划分，则全期望公式可以被定义为：</p>
<script type="math/tex; mode=display">
\mathbb{E}[X]=\sum_i\mathbb{E}\left[X\mid A_i\right]p\left(A_i\right)</script><p>现在证明：$\mathbb{E}[V(s_{t+1})|s_t]=\mathbb{E}[\mathbb{E}[G_{t+1}|s_{t+1}]|s_t]=\mathbb{E}[G_{t+1}|s_t]$</p>
<h4 id="贝尔曼方程的推导"><a href="#贝尔曼方程的推导" class="headerlink" title="贝尔曼方程的推导"></a>贝尔曼方程的推导</h4><h2 id="马尔可夫决策过程-1"><a href="#马尔可夫决策过程-1" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h2></article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/avatar.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/avatar.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">11andyy</div><div class="post-copyright__author_desc">生活明朗，万物可爱</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://11andyy.github.io/2024/08/01/AutonomousDriving/easyRL/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://11andyy.github.io/2024/08/01/AutonomousDriving/easyRL/')">强化学习</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="/img/wechatpay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/wechatpay.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://11andyy.github.io/2024/08/01/AutonomousDriving/easyRL/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=强化学习&amp;url=https://11andyy.github.io/2024/08/01/AutonomousDriving/easyRL/&amp;pic=https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/d7b8a1da4176723d2a85993fae53b10a37d7ea4f.png" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://11andyy.github.io" target="_blank">11的Blog</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>自动驾驶<span class="tagsPageCount">4</span></a><a class="post-meta__box__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>强化学习<span class="tagsPageCount">1</span></a><a class="post-meta__box__tags" href="/tags/easyRL%E8%98%91%E8%8F%87%E4%B9%A6/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>easyRL蘑菇书<span class="tagsPageCount">1</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20241026155828470.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/30/AutonomousDriving/carla/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240807120431221.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Carla</div></div></a></div><div class="next-post pull-right"><a href="/2024/08/07/Tutorial/WSL%E5%AE%89%E8%A3%85/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20240807105753132.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">WSL2安装及其配置</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/2024/10/06/AutonomousDriving/PathPlanning/" title="路径规划"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20241006150221350.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2024-10-06</div><div class="title">路径规划</div></div></a></div><div><a href="/2024/10/16/AutonomousDriving/GNSS/" title="GNSS与惯性及多传感器组合导航系统原理"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20241018181640482.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2024-10-16</div><div class="title">GNSS与惯性及多传感器组合导航系统原理</div></div></a></div><div><a href="/2024/07/29/AutonomousDriving/ros/" title="Ros"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/_imkt-WQ.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2024-07-29</div><div class="title">Ros</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png" alt="status"/></div></div><div class="author-info__description"><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">本博客建立的初衷是为了记录一路走来学习的计算机专业知识,方便之后复习与查看,<b style="color:#fff">起于此，但不止于此</b>,勤能补拙，相信一点点的积累最后汇聚成海!</div><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">希望我的这个小小的计划,可以真正地帮助到实力强大的你!</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">11andyy</h1><div class="author-info__desc">生活明朗，万物可爱</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/11andyy" target="_blank" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="social-icon faa-parent animated-hover" href="https://space.bilibili.com/372204786" target="_blank" title="BiliBili"><i class="anzhiyufont anzhiyu-icon-bilibili"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来看我的博客鸭~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">强化学习基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">强化学习概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.1.1.</span> <span class="toc-text">强化学习与监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">1.1.2.</span> <span class="toc-text">强化学习的应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%86%B3%E7%AD%96"><span class="toc-number">1.2.</span> <span class="toc-text">序列决策</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%E4%B8%8E%E7%8E%AF%E5%A2%83"><span class="toc-number">1.2.1.</span> <span class="toc-text">智能体与环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A5%96%E5%8A%B1"><span class="toc-number">1.2.2.</span> <span class="toc-text">奖励</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%86%B3%E7%AD%96-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">序列决策</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E4%BD%9C%E7%A9%BA%E9%97%B4"><span class="toc-number">1.3.</span> <span class="toc-text">动作空间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%BB%84%E6%88%90%E6%88%90%E5%88%86%E5%92%8C%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.4.</span> <span class="toc-text">强化学习智能体的组成成分和类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AD%96%E7%95%A5"><span class="toc-number">1.4.1.</span> <span class="toc-text">策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.2.</span> <span class="toc-text">价值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.4.3.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.4.4.</span> <span class="toc-text">强化学习智能体的类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BB%B7%E5%80%BC%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93%E4%B8%8E%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-number">1.4.4.1.</span> <span class="toc-text">基于价值的智能体与基于策略的智能体</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%89%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93%E4%B8%8E%E5%85%8D%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-number">1.4.4.2.</span> <span class="toc-text">有模型强化学习智能体与免模型强化学习智能体</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E5%92%8C%E8%A7%84%E5%88%92"><span class="toc-number">1.5.</span> <span class="toc-text">学习和规划</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8"><span class="toc-number">1.6.</span> <span class="toc-text">探索和利用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C%E5%B9%B3%E5%8F%B0"><span class="toc-number">1.7.</span> <span class="toc-text">强化学习实验平台</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Gym"><span class="toc-number">1.7.1.</span> <span class="toc-text">Gym</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MountainCar-v0"><span class="toc-number">1.7.2.</span> <span class="toc-text">MountainCar-v0</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E8%AF%8D"><span class="toc-number">1.8.</span> <span class="toc-text">关键词</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">马尔可夫决策过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%EF%BC%88MP%EF%BC%89"><span class="toc-number">2.1.</span> <span class="toc-text">马尔可夫过程（MP）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8"><span class="toc-number">2.1.1.</span> <span class="toc-text">马尔可夫性质</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE"><span class="toc-number">2.1.2.</span> <span class="toc-text">马尔可夫链</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8B%EF%BC%88MRP%EF%BC%89"><span class="toc-number">2.2.</span> <span class="toc-text">马尔可夫奖励过程（MRP）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E6%8A%A5%E4%B8%8E%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.1.</span> <span class="toc-text">回报与价值函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B"><span class="toc-number">2.2.2.</span> <span class="toc-text">贝尔曼方程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E6%9C%9F%E6%9C%9B%E5%85%AC%E5%BC%8F"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">全期望公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="toc-number">2.2.2.2.</span> <span class="toc-text">贝尔曼方程的推导</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B-1"><span class="toc-number">2.3.</span> <span class="toc-text">马尔可夫决策过程</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/10/20/WeeklyReport/PaperReading/" title="PaperReading"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20241026155828470.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PaperReading"/></a><div class="content"><a class="title" href="/2024/10/20/WeeklyReport/PaperReading/" title="PaperReading">PaperReading</a><time datetime="2024-10-20T03:58:03.000Z" title="发表于 2024-10-20 11:58:03">2024-10-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/20/WeeklyReport/%E5%91%A8%E8%AE%B0/" title="周报"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20241026155828470.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="周报"/></a><div class="content"><a class="title" href="/2024/10/20/WeeklyReport/%E5%91%A8%E8%AE%B0/" title="周报">周报</a><time datetime="2024-10-20T03:58:03.000Z" title="发表于 2024-10-20 11:58:03">2024-10-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/16/AutonomousDriving/GNSS/" title="GNSS与惯性及多传感器组合导航系统原理"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20241018181640482.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GNSS与惯性及多传感器组合导航系统原理"/></a><div class="content"><a class="title" href="/2024/10/16/AutonomousDriving/GNSS/" title="GNSS与惯性及多传感器组合导航系统原理">GNSS与惯性及多传感器组合导航系统原理</a><time datetime="2024-10-16T04:02:03.000Z" title="发表于 2024-10-16 12:02:03">2024-10-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/06/AutonomousDriving/PathPlanning/" title="路径规划"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20241006150221350.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="路径规划"/></a><div class="content"><a class="title" href="/2024/10/06/AutonomousDriving/PathPlanning/" title="路径规划">路径规划</a><time datetime="2024-10-06T07:02:03.000Z" title="发表于 2024-10-06 15:02:03">2024-10-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/06/Tools/CMake/" title="CMake"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://cdn.jsdelivr.net/gh/11andyy/Pic0@master/img/image-20241006084123871.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CMake"/></a><div class="content"><a class="title" href="/2024/10/06/Tools/CMake/" title="CMake">CMake</a><time datetime="2024-10-06T00:02:03.000Z" title="发表于 2024-10-06 08:02:03">2024-10-06</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v7.3.0" title="博客框架为Hexo_v7.3.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Frame-Hexo.svg" alt="博客框架为Hexo_v7.3.0"/></a><a class="github-badge" target="_blank" href="https://blog.anheyu.com/" style="margin-inline:5px" data-title="本站使用AnZhiYu主题" title="本站使用AnZhiYu主题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.9/img/Theme-AnZhiYu-2E67D3.svg" alt="本站使用AnZhiYu主题"/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Source-Github.svg" alt="本站项目由Github托管"/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/Copyright-BY-NC-SA.svg" alt="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"/></a></p></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2021 - 2024 By <a class="footer-bar-link" href="/" title="11andyy" target="_blank">11andyy</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">44</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">7</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://index.anheyu.com/" title="个人主页"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/13/64d8c2748ef34.jpg" alt="个人主页"/><span class="back-menu-item-text">个人主页</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/07/23/64bc72c75319d.png" alt="博客"/><span class="back-menu-item-text">博客</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/13/64d8c2653332e.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/13/64d8c2653332e.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/408/" style="font-size: 0.88rem;">408<sup>4</sup></a><a href="/tags/Anaconda/" style="font-size: 0.88rem;">Anaconda<sup>1</sup></a><a href="/tags/C/" style="font-size: 0.88rem;">C++<sup>1</sup></a><a href="/tags/CMake/" style="font-size: 0.88rem;">CMake<sup>1</sup></a><a href="/tags/Carla/" style="font-size: 0.88rem;">Carla<sup>1</sup></a><a href="/tags/Docker/" style="font-size: 0.88rem;">Docker<sup>1</sup></a><a href="/tags/GNSS/" style="font-size: 0.88rem;">GNSS<sup>1</sup></a><a href="/tags/Git/" style="font-size: 0.88rem;">Git<sup>1</sup></a><a href="/tags/Latex/" style="font-size: 0.88rem;">Latex<sup>1</sup></a><a href="/tags/Linux/" style="font-size: 0.88rem;">Linux<sup>1</sup></a><a href="/tags/Pipe/" style="font-size: 0.88rem;">Pipe<sup>1</sup></a><a href="/tags/Python/" style="font-size: 0.88rem;">Python<sup>1</sup></a><a href="/tags/Ros/" style="font-size: 0.88rem;">Ros<sup>1</sup></a><a href="/tags/SCP/" style="font-size: 0.88rem;">SCP<sup>1</sup></a><a href="/tags/SSH/" style="font-size: 0.88rem;">SSH<sup>1</sup></a><a href="/tags/Tmux/" style="font-size: 0.88rem;">Tmux<sup>2</sup></a><a href="/tags/Vim/" style="font-size: 0.88rem;">Vim<sup>2</sup></a><a href="/tags/WSL/" style="font-size: 0.88rem;">WSL<sup>1</sup></a><a href="/tags/easyRL%E8%98%91%E8%8F%87%E4%B9%A6/" style="font-size: 0.88rem;">easyRL蘑菇书<sup>1</sup></a><a href="/tags/%E5%9B%BE%E5%BA%8A/" style="font-size: 0.88rem;">图床<sup>1</sup></a><a href="/tags/%E5%AE%9E%E7%94%A8%E6%95%99%E7%A8%8B/" style="font-size: 0.88rem;">实用教程<sup>4</sup></a><a href="/tags/%E5%AF%BC%E8%88%AA%E5%AE%9A%E4%BD%8D/" style="font-size: 0.88rem;">导航定位<sup>1</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7/" style="font-size: 0.88rem;">工具<sup>1</sup></a><a href="/tags/%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/" style="font-size: 0.88rem;">工具配置<sup>1</sup></a><a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem;">强化学习<sup>1</sup></a><a href="/tags/%E6%83%AF%E6%80%A7%E5%AF%BC%E8%88%AA/" style="font-size: 0.88rem;">惯性导航<sup>1</sup></a><a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 0.88rem;">操作系统<sup>2</sup></a><a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 0.88rem;">数据结构<sup>1</sup></a><a href="/tags/%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%B7%A5%E5%85%B7/" style="font-size: 0.88rem;">文本编辑工具<sup>2</sup></a><a href="/tags/%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 0.88rem;">机器人操作系统<sup>1</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7/" style="font-size: 0.88rem;">深度学习工具<sup>5</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 0.88rem;">算法<sup>1</sup></a><a href="/tags/%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5/" style="font-size: 0.88rem;">组会汇报<sup>2</sup></a><a href="/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" style="font-size: 0.88rem;">编程语言<sup>1</sup></a><a href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/" style="font-size: 0.88rem;">自动驾驶<sup>4</sup></a><a href="/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E4%BB%BF%E7%9C%9F/" style="font-size: 0.88rem;">自动驾驶仿真<sup>1</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/" style="font-size: 0.88rem;">计算机组成原理<sup>1</sup></a><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" style="font-size: 0.88rem;">计算机网络<sup>1</sup></a><a href="/tags/%E8%AE%BA%E6%96%87/" style="font-size: 0.88rem;">论文<sup>1</sup></a><a href="/tags/%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92/" style="font-size: 0.88rem;">路径规划<sup>1</sup></a></div></div><hr/></div></div><div id="keyboard-tips"><div class="keyboardTitle">博客快捷键</div><div class="keybordList"><div class="keybordItem"><div class="keyGroup"><div class="key">shift K</div></div><div class="keyContent"><div class="content">关闭快捷键功能</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift A</div></div><div class="keyContent"><div class="content">打开/关闭中控台</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift M</div></div><div class="keyContent"><div class="content">播放/暂停音乐</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift D</div></div><div class="keyContent"><div class="content">深色/浅色显示模式</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift S</div></div><div class="keyContent"><div class="content">站内搜索</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift R</div></div><div class="keyContent"><div class="content">随机访问</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift H</div></div><div class="keyContent"><div class="content">返回首页</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift F</div></div><div class="keyContent"><div class="content">友链鱼塘</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift L</div></div><div class="keyContent"><div class="content">友链页面</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift P</div></div><div class="keyContent"><div class="content">关于本站</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift I</div></div><div class="keyContent"><div class="content">原版/本站右键菜单</div></div></div></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="anzhiyufont anzhiyu-icon-comment-sms"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2021 By 安知鱼 V1.6.12",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 11andyy 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.cbd.int/mathjax@3.2.2/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><input type="hidden" name="page-type" id="page-type" value="post"></div><script async data-pjax src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.1/bubble/bubble.js"></script><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><script>(() => {
  const isChatBtn = true
  const isChatHideShow = false

  if (isChatBtn) {
    const close = () => {
      Chatra('minimizeWidget')
      Chatra('hide')
    }

    const open = () => {
      Chatra('openChat', true)
      Chatra('show')
    }

    window.ChatraSetup = {
      startHidden: true
    }
  
    window.chatBtnFn = () => {
      const isShow = document.getElementById('chatra').classList.contains('chatra--expanded')
      isShow ? close() : open()
    }
  } else if (isChatHideShow) {
    window.chatBtn = {
      hide: () => {
        Chatra('hide')
      },
      show: () => {
        Chatra('show')
      }
    }
  }

  (function(d, w, c) {
    w.ChatraID = 'XuXAxHxmtsvpjunNh'
    var s = d.createElement('script')
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments)
    }
    s.async = true
    s.src = 'https://call.chatra.io/chatra.js'
    if (d.head) d.head.appendChild(s)
  })(document, window, 'Chatra')

})()</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>